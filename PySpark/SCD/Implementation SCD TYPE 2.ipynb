{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d09a2d7-46a3-4106-b5a8-f7c0a118a55b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window, functions as F\n",
    "from datetime import timedelta,datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e390b8-0a58-43e6-b965-bdca340056dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "        (1, 'James','Driver',15,datetime.today()-timedelta(days = 12)),\n",
    "        (1, 'James','Teacher',18,datetime.today() - timedelta(days = 10)),\n",
    "        (1 , 'James','Engineer',23,datetime.today()-timedelta(days = 8)),\n",
    "        (4,'Abhishek','architect',28,datetime.today()-timedelta(days = 7)),\n",
    "        (5,'Jeefron','CEO',67,datetime.today()-timedelta(days=6))\n",
    "      ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf517e79-e0a3-4066-8409-edd93c40d755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data = data ,schema = ['id','name','occupation','age','date_column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e94298fc-0404-4274-843c-8db477eba7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class SCD2:\n",
    "    def __init__(self, primary_keys: list[str], order_column: str, delete_condition: str):\n",
    "        self.primary_keys = primary_keys\n",
    "        self.order_column = order_column\n",
    "        self.delete_condition = delete_condition\n",
    "\n",
    "    def apply_scd2(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Apply SCD2 transformations to the source DataFrame.\n",
    "        \"\"\"\n",
    "        window_spec = Window.partitionBy(*self.primary_keys).orderBy(self.order_column)\n",
    "\n",
    "        # Step 1: Add DateTimeValidFrom and DateTimeValidTo to a struct\n",
    "        df = df.withColumn(\n",
    "            \"__versioned\",\n",
    "            F.struct(\n",
    "                F.col(self.order_column).alias(\"DateTimeValidFrom\"),\n",
    "                F.lead(self.order_column).over(window_spec).alias(\"DateTimeValidTo\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Step 2: Adjust DateTimeValidTo with a 1-second lag\n",
    "        df = df.withColumn(\n",
    "            \"__versioned\",\n",
    "            F.col(\"__versioned\").withField(\n",
    "                \"DateTimeValidTo\",\n",
    "                F.when(F.col(\"__versioned.DateTimeValidTo\").isNotNull(),\n",
    "                       F.col(\"__versioned.DateTimeValidTo\") - F.expr(\"INTERVAL 1 SECOND\")\n",
    "                      ).otherwise(F.lit(None))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Step 3: Add DeleteFlag\n",
    "        df = df.withColumn(\n",
    "            \"__versioned\",\n",
    "            F.col(\"__versioned\").withField(\n",
    "                \"DeleteFlag\",\n",
    "                F.when(F.expr(self.delete_condition), F.lit(True)).otherwise(F.lit(False))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Step 4: Update DateTimeValidTo for deleted records\n",
    "        df = df.withColumn(\n",
    "            \"__versioned\",\n",
    "            F.col(\"__versioned\").withField(\n",
    "                \"DateTimeValidTo\",\n",
    "                F.when(\n",
    "                    (F.col(\"__versioned.DeleteFlag\") == True) &\n",
    "                    F.col(\"__versioned.DateTimeValidTo\").isNull(),\n",
    "                    F.current_timestamp()\n",
    "                ).otherwise(F.col(\"__versioned.DateTimeValidTo\"))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def catch(self, target_table: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the target table exists in the catalog.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            spark.read.table(target_table)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def execute(self, df: DataFrame, target_table: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Execute the SCD2 process, merging the source DataFrame with the target table.\n",
    "        \"\"\"\n",
    "        # Apply SCD2 transformations to the source DataFrame\n",
    "        source_df = self.apply_scd2(df)\n",
    "\n",
    "        # Check if target table exists\n",
    "        if self.catch(target_table):\n",
    "            # Load the target table\n",
    "            target_df = spark.read.table(target_table)\n",
    "        else:\n",
    "            # Create an empty DataFrame with the source DataFrame schema if table doesn't exist\n",
    "            target_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), source_df.schema)\n",
    "\n",
    "        # Union source and target DataFrames\n",
    "        unioned_df = target_df.unionByName(source_df, allowMissingColumns=True)\n",
    "\n",
    "        # Reapply SCD2 transformations after the union\n",
    "        final_df = self.apply_scd2(unioned_df)\n",
    "\n",
    "        return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26bb59aa-cd07-4589-82db-856a03f029ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>occupation</th><th>age</th><th>date_column</th><th>__versioned</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>Engineer</td><td>30</td><td>2023-11-19 10:00:00</td><td>List(2023-11-19 10:00:00, 2024-11-08 13:56:24.79351, false)</td></tr><tr><td>1</td><td>James</td><td>Driver</td><td>15</td><td>2024-11-08 13:56:25.79351</td><td>List(2024-11-08 13:56:25.79351, 2024-11-10 13:56:24.793532, false)</td></tr><tr><td>1</td><td>James</td><td>Teacher</td><td>18</td><td>2024-11-10 13:56:25.793532</td><td>List(2024-11-10 13:56:25.793532, 2024-11-12 13:56:24.793564, false)</td></tr><tr><td>1</td><td>James</td><td>Engineer</td><td>23</td><td>2024-11-12 13:56:25.793564</td><td>List(2024-11-12 13:56:25.793564, null, false)</td></tr><tr><td>2</td><td>Bob</td><td>Doctor</td><td>35</td><td>2023-11-19 11:00:00</td><td>List(2023-11-19 11:00:00, null, false)</td></tr><tr><td>3</td><td>Charlie</td><td>Artist</td><td>25</td><td>2023-11-19 12:00:00</td><td>List(2023-11-19 12:00:00, null, false)</td></tr><tr><td>4</td><td>Abhishek</td><td>architect</td><td>28</td><td>2024-11-13 13:56:25.793571</td><td>List(2024-11-13 13:56:25.793571, null, false)</td></tr><tr><td>5</td><td>Eve</td><td>Scientist</td><td>40</td><td>2023-11-19 13:00:00</td><td>List(2023-11-19 13:00:00, 2024-11-14 13:56:24.793575, true)</td></tr><tr><td>5</td><td>Jeefron</td><td>CEO</td><td>67</td><td>2024-11-14 13:56:25.793575</td><td>List(2024-11-14 13:56:25.793575, 2024-11-20 13:56:40.086, true)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice",
         "Engineer",
         30,
         "2023-11-19 10:00:00",
         [
          "2023-11-19 10:00:00",
          "2024-11-08 13:56:24.79351",
          false
         ]
        ],
        [
         1,
         "James",
         "Driver",
         15,
         "2024-11-08 13:56:25.79351",
         [
          "2024-11-08 13:56:25.79351",
          "2024-11-10 13:56:24.793532",
          false
         ]
        ],
        [
         1,
         "James",
         "Teacher",
         18,
         "2024-11-10 13:56:25.793532",
         [
          "2024-11-10 13:56:25.793532",
          "2024-11-12 13:56:24.793564",
          false
         ]
        ],
        [
         1,
         "James",
         "Engineer",
         23,
         "2024-11-12 13:56:25.793564",
         [
          "2024-11-12 13:56:25.793564",
          null,
          false
         ]
        ],
        [
         2,
         "Bob",
         "Doctor",
         35,
         "2023-11-19 11:00:00",
         [
          "2023-11-19 11:00:00",
          null,
          false
         ]
        ],
        [
         3,
         "Charlie",
         "Artist",
         25,
         "2023-11-19 12:00:00",
         [
          "2023-11-19 12:00:00",
          null,
          false
         ]
        ],
        [
         4,
         "Abhishek",
         "architect",
         28,
         "2024-11-13 13:56:25.793571",
         [
          "2024-11-13 13:56:25.793571",
          null,
          false
         ]
        ],
        [
         5,
         "Eve",
         "Scientist",
         40,
         "2023-11-19 13:00:00",
         [
          "2023-11-19 13:00:00",
          "2024-11-14 13:56:24.793575",
          true
         ]
        ],
        [
         5,
         "Jeefron",
         "CEO",
         67,
         "2024-11-14 13:56:25.793575",
         [
          "2024-11-14 13:56:25.793575",
          "2024-11-20 13:56:40.086",
          true
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "occupation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "date_column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "__versioned",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"DateTimeValidFrom\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DateTimeValidTo\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeleteFlag\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}}]}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame with the sample data\n",
    "df = spark.createDataFrame(data, schema=['id', 'name', 'occupation', 'age', 'date_column'])\n",
    "\n",
    "# Initialize the SCD2 object\n",
    "scd2_object = SCD2([\"id\"], \"date_column\", \"id = 5\")\n",
    "\n",
    "# Specify the target table name\n",
    "target_table_name = \"target_table\"\n",
    "\n",
    "# Execute the SCD2 process\n",
    "df_final = scd2_object.execute(df, target_table_name)\n",
    "\n",
    "# Save the final DataFrame to the target table\n",
    "df_final.write.mode('overwrite').saveAsTable(target_table_name)\n",
    "\n",
    "# Display the saved target table\n",
    "spark.read.table(target_table_name).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8258d333-2798-4a6f-a52f-bab2aef10970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Implementation SCD TYPE 2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
